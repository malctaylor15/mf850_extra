library(MASS)
gamma_params <- fitdistr(Sn_MC_samples, "gamma")
gamma_shape <- gamma_params$estimate[1]
gamma_rate <- gamma_params$estimate[2]
gamma_tail_approx <- qgamma(quantiles,shape = gamma_shape, rate = gamma_rate )
gamma_tail <- 1- gamma_tail_approx
# Plot on log log scale
plot(quantiles,emp_tail, log = "xy", type = 'l', col = 'red', lwd = 2)
lines(quantiles, norm_tail, type = 'l', col = 'green', lwd = 2)
lines(quantiles, gamma_tail, type = 'l', col = 'orange', lwd = 2)
legend("bottomleft", c("Empirical", "Normal", "Gamma"), lwd = 2, col = c("red", "green", "orange"))
min(emp_tail)
emp_tail ==0
sum(emp_tail ==0)
log(0)
library(MASS)
gamma_params <- fitdistr(Sn_MC_samples, "gamma")
gamma_shape <- gamma_params$estimate[1]
gamma_rate <- gamma_params$estimate[2]
gamma_tail_approx <- qgamma(quantiles,shape = gamma_shape, rate = gamma_rate )
gamma_tail <- 1- gamma_tail_approx
# Plot on log log scale
plot(quantiles,emp_tail, log = "xy", type = 'l', col = 'red', lwd = 2)
lines(quantiles, norm_tail, type = 'l', col = 'green', lwd = 2)
lines(quantiles, gamma_tail, type = 'l', col = 'orange', lwd = 2)
legend("bottomleft", c("Empirical", "Normal", "Gamma"), lwd = 2, col = c("red", "green", "orange"))
plot(quantiles,emp_tail, type = 'l', col = 'red', lwd = 2)
lines(quantiles, norm_tail, type = 'l', col = 'green', lwd = 2)
lines(quantiles, gamma_tail, type = 'l', col = 'orange', lwd = 2)
X1 <- rpois(1000,9) # Generate random variable- try other random variables
EX1 <- mean(X1) # E[X] - first moment
VarX1 <- var(X1) # Variance = E[X^2] - E[X]^2
mean2 <- EX1^2 # E[X]^2
(EX2_hypo <- mean(X1^2) ) # E[X^2] ? hypothesis
(EX2_act <- VarX1 + mean2) # E[X^2] = Var(X) - E[X]^2
log(1)
# Approximating a compound poisson random variable
numb_sn <- 100000 # M
lambda <- 100 # Poisson process parameter
mu = 0.10 # Parameter for X
sigma = 0.4 # Parameter for X
# Begin approximating Sn
N_vector <- rpois(numb_sn, lambda)
# Compute a sample of Sn
Sn_sample <- function(N, mean = mu, std = sigma){
log_X_k <- rnorm(N, mean = mu, sd = sigma)
X_k <- exp(log_X_k)
Sn <- sum(X_k)
return (Sn)
}
# Pre allocate space
Sn_MC_samples <- rep(0, numb_sn)
# Compute M copies of Sn
for (sample in 1:numb_sn){
N_i <- N_vector[sample]
Sn_MC_samples[sample] <- Sn_sample(N_i, mu, sigma)
}
# Plot the distribution with a normal density overlaid
makeHist <- function(x, color = "blue", title = "Histogram"){
h<-hist(x,breaks = 100 ,main=title)
xfit<-seq(min(x),max(x),length=100)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col=color, lwd=2)
}
makeHist(Sn_MC_samples)
####################### Quantile Stuff #############################
#################################################################
quantiles <- seq(from = 0.95, to = 0.9999, by = 0.0001)
# Empirical distribution quantiles
empirical_tail <- quantile(Sn_MC_samples, quantiles)
emp_tail <- 1-empirical_tail
# Normal Approximation
Sn_mean <- mean(Sn_MC_samples)
Sn_sd <- sd(Sn_MC_samples)
norm_tail_approx <- qnorm(quantiles, Sn_mean, Sn_sd)
norm_tail <- 1- norm_tail_approx
# Gamma tail approx ... Equations from the book - pg 477 of pdf
# This is not the 3 parameter gamma....
library(MASS)
gamma_params <- fitdistr(Sn_MC_samples, "gamma")
gamma_shape <- gamma_params$estimate[1]
gamma_rate <- gamma_params$estimate[2]
gamma_tail_approx <- qgamma(quantiles,shape = gamma_shape, rate = gamma_rate )
gamma_tail <- 1- gamma_tail_approx
# Need to plot on log log
# Plot on log log scale
plot(quantiles,emp_tail, log = "xy", type = 'l', col = 'red', lwd = 2)
lines(quantiles, norm_tail, type = 'l', col = 'green', lwd = 2)
lines(quantiles, gamma_tail, type = 'l', col = 'orange', lwd = 2)
legend("bottomleft", c("Empirical", "Normal", "Gamma"), lwd = 2, col = c("red", "green", "orange"))
min(log(emp_tail))
sum(is.na(emp_tail))
plot(emp_tail)
log(min(emp_tail))
min(emp_tail)
log(-1)
plot(empirical_tail)
emp_tail <- empirical_tail
emp_tail <- empirical_tail # supposed to be 1-Fsn
norm_tail <- norm_tail_approx
plot(quantiles,emp_tail, log = "xy", type = 'l', col = 'red', lwd = 2)
norm_tail <- norm_tail_approx # Supposed to be 1-Fsn
gamma_tail <- gamma_tail_approx # Supposed to be 1- Fsn
# Need to plot on log log
# Plot on log log scale
plot(quantiles,emp_tail, log = "xy", type = 'l', col = 'red', lwd = 2)
lines(quantiles, norm_tail, type = 'l', col = 'green', lwd = 2)
lines(quantiles, gamma_tail, type = 'l', col = 'orange', lwd = 2)
legend("bottomleft", c("Empirical", "Normal", "Gamma"), lwd = 2, col = c("red", "green", "orange"))
quantiles <- seq(from = 0.95, to = 0.9999, by = 0.0001)
# Empirical distribution quantiles
empirical_tail <- quantile(Sn_MC_samples, quantiles)
emp_tail <- 1 - empirical_tail
# Normal Approximation
Sn_mean <- mean(Sn_MC_samples)
Sn_sd <- sd(Sn_MC_samples)
norm_tail_approx <- qnorm(quantiles, Sn_mean, Sn_sd)
norm_tail <- 1- norm_tail_approx
# Gamma tail approx ... Equations from the book - pg 477 of pdf
# Or simplified equations on 497 in appendix
# This is not the 3 parameter gamma....
library(MASS)
gamma_params <- fitdistr(Sn_MC_samples, "gamma")
gamma_shape <- gamma_params$estimate[1]
gamma_rate <- gamma_params$estimate[2]
gamma_tail_approx <- qgamma(quantiles,shape = gamma_shape, rate = gamma_rate )
gamma_tail <- 1- gamma_tail_approx
# Need to plot on log log
# Plot on log log scale
# NOT ON LOG LOG SCALE
plot(quantiles,emp_tail, type = 'l', col = 'red', lwd = 2)
lines(quantiles, norm_tail, type = 'l', col = 'green', lwd = 2)
lines(quantiles, gamma_tail, type = 'l', col = 'orange', lwd = 2)
legend("bottomleft", c("Empirical", "Normal", "Gamma"), lwd = 2, col = c("red", "green", "orange"))
plot(quantiles,emp_tail, type = 'l', col = 'red', lwd = 2)
lines(quantiles, norm_tail, type = 'l', col = 'blue', lwd = 2)
lines(quantiles, gamma_tail, type = 'l', col = 'orange', lwd = 2)
legend("bottomleft", c("Empirical", "Normal", "Gamma"), lwd = 2, col = c("red", "blue", "orange"))
plot(emp_tail, quantiles, type = 'l', col = 'red', lwd = 2)
lines(norm_tail, quantiles, type = 'l', col = 'blue', lwd = 2)
lines(gamma_tail, quantiles, type = 'l', col = 'orange', lwd = 2)
legend("bottomleft", c("Empirical", "Normal", "Gamma"), lwd = 2, col = c("red", "blue", "orange"))
plot(emp_tail, log(quantiles), type = 'l', col = 'red', lwd = 2)
lines(norm_tail, log(quantiles), type = 'l', col = 'blue', lwd = 2)
lines(gamma_tail, log(quantiles), type = 'l', col = 'orange', lwd = 2)
legend("bottomleft", c("Empirical", "Normal", "Gamma"), lwd = 2, col = c("red", "blue", "orange"))
plot(emp_tail, quantiles, log = "y", type = 'l', col = 'red', lwd = 2)
plot(emp_tail, log(quantiles), log = "y", type = 'l', col = 'red', lwd = 2)
plot(emp_tail, quantiles, log = "y", type = 'l', col = 'red', lwd = 2)
plot(emp_tail, quantiles, log = "xy", type = 'l', col = 'red', lwd = 2)
plot(emp_tail, quantiles, type = 'l', col = 'red', lwd = 2)
lines(norm_tail, quantiles, type = 'l', col = 'blue', lwd = 2)
lines(gamma_tail, quantiles, type = 'l', col = 'orange', lwd = 2)
legend("bottomleft", c("Empirical", "Normal", "Gamma"), lwd = 2, col = c("red", "blue", "orange"))
qnorm(0.99)
qnorm(0.99)
qnorm(0.01)
exp(0.01)
# Approximating a compound poisson random variable
numb_sn <- 100000 # M
lambda <- 100 # Poisson process parameter
mu = 0.10 # Parameter for X
sigma = 0.4 # Parameter for X
# Begin approximating Sn
# Approximating a compound poisson random variable
numb_sn <- 100000 # M
lambda <- 100 # Poisson process parameter
mu = 0.10 # Parameter for X
sigma = 0.4 # Parameter for X
# Begin approximating Sn
N_vector <- rpois(numb_sn, lambda)
# Compute a sample of Sn
Sn_sample <- function(N, mean = mu, std = sigma){
log_X_k <- rnorm(N, mean = mu, sd = sigma)
X_k <- exp(log_X_k)
Sn <- sum(X_k)
return (Sn)
}
# Pre allocate space
Sn_MC_samples <- rep(0, numb_sn)
# Compute M copies of Sn
for (sample in 1:numb_sn){
N_i <- N_vector[sample]
Sn_MC_samples[sample] <- Sn_sample(N_i, mu, sigma)
}
# Plot the distribution with a normal density overlaid
makeHist <- function(x, color = "blue", title = "Histogram"){
h<-hist(x,breaks = 100 ,main=title)
xfit<-seq(min(x),max(x),length=100)
yfit<-dnorm(xfit,mean=mean(x),sd=sd(x))
yfit <- yfit*diff(h$mids[1:2])*length(x)
lines(xfit, yfit, col=color, lwd=2)
}
makeHist(Sn_MC_samples)
####################### Quantile Stuff #############################
#################################################################
quantiles <- seq(from = 0.95, to = 0.9999, by = 0.0001)
# Empirical distribution quantiles
empirical_tail <- quantile(Sn_MC_samples, quantiles)
emp_tail <- 1 - empirical_tail
# Normal Approximation quantiles
Sn_mean <- mean(Sn_MC_samples)
Sn_sd <- sd(Sn_MC_samples)
norm_tail_approx <- qnorm(quantiles, Sn_mean, Sn_sd)
norm_tail <- 1- norm_tail_approx
# Gamma tail approx ... Equations from the book - pg 477 of pdf
# Or simplified equations on 497 in appendix
# This is not the 3 parameter gamma....
# Find the parameters using MLE in the MASS package
library(MASS)
gamma_params <- fitdistr(Sn_MC_samples, "gamma")
gamma_shape <- gamma_params$estimate[1]
gamma_rate <- gamma_params$estimate[2]
# Compute the quantiles and find the tail distribution
gamma_tail_approx <- qgamma(quantiles,shape = gamma_shape, rate = gamma_rate )
gamma_tail <- 1- gamma_tail_approx
# Plot on log log scale
# NOT ON LOG LOG SCALE
plot(quantiles,emp_tail, type = 'l', col = 'red', lwd = 2)
lines(quantiles, norm_tail, type = 'l', col = 'blue', lwd = 2)
lines(quantiles, gamma_tail, type = 'l', col = 'orange', lwd = 2)
legend("bottomleft", c("Empirical", "Normal", "Gamma"), lwd = 2, col = c("red", "blue", "orange"))
# The approximations should not straddle the empirical distribution
# Plotting again to look more like his but same analytical problems
# Switch x and y axis
plot(emp_tail, quantiles, type = 'l', col = 'red', lwd = 2)
lines(norm_tail, quantiles, type = 'l', col = 'blue', lwd = 2)
lines(gamma_tail, quantiles, type = 'l', col = 'orange', lwd = 2)
legend("bottomleft", c("Empirical", "Normal", "Gamma"), lwd = 2, col = c("red", "blue", "orange"))
??regsubset
# Stepwise Variable selection
setwd("C:/Users/board/Desktop/Kaggle/mf850_extra")
data <- read.csv("mf850-loan-data.csv")
set.seed(1)
# Data cleaning - change variables into categorical variables
data$DEPENDENTS <- as.factor(data$DEPENDENTS)
data$CRED_HERE <- as.factor(data$CRED_HERE)
data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
# Scale continous variables
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
# Splitting the data into test/ train sets
# https://ragrawal.wordpress.com/2012/01/14/dividing-data-into-training-and-testing-dataset-in-r/
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
# Try backwards stepwise regression with logistic regression
# Inspiration
# http://www.utstat.toronto.edu/~brunner/oldclass/appliedf11/handouts/2101f11StepwiseLogisticR.pdf
# Logistic regression with all variables
fullmod <- glm(CRED_APPROVED~. , family = "binomial", data =train)
summary(fullmod)
# Logisitic regression with no variables
nothing <- glm(CRED_APPROVED~1, family = "binomial", data = train)
summary(nothing) # Essentially intercept says yes (it is greater than 0.5)
# Backward stepwise regression
backwards <- step(fullmod, direction = "backward", trace = 0)
summary(backwards)
length(backwards$coefficients)
# Great only 35 variables compared to 56
# Let's see how it does on the prediction set
predict_fit2 <- predict(backwards, newdata=test, type = "response")
hist(predict_fit2, breaks = 20)
# Use 50% threshold for predictions ( we can try different thresholds later )
results2  <- ifelse(predict_fit2 > 0.5,'YES','NO')
# Compare with the original results
misClasificError <- mean(results2 != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Get the variables from backward selection
# Split categorical variables into seperate columns
x = model.matrix(CRED_APPROVED~., data = train)
# Re structure data so the data so it is a data frame and not a matrix
data3 <- data.frame(x)
# Save names of the coefficients in the backward step wise regression
coef_names <- names(backwards$coefficients)
# Delete the first name which is X intercept
coef_names <- coef_names[-1]
# Make sure all the variables are in the data set
data_col_names <- colnames(data3)
sum(coef_names%in% data_col_names) == (length(coef_names))
# Get the columns from the new data set
data_rf <- data3[,coef_names]
data1 <- cbind(data_rf, y)
# Random forest with that data
library(randomForest)
fit_rf2 <- randomForest(as.factor(y)~. , data = data1)
fit_rf2
importance(fit_rf2)
# Create data frame to store ntree solving results
ntree_df <- data.frame(matrix(0, nrow= 50, ncol = 4))
y = ifelse(train$CRED_APPROVED == 'YES',1,0)
data1 <- cbind(data_rf, y)
# Random forest with that data
library(randomForest)
fit_rf2 <- randomForest(as.factor(y)~. , data = data1)
fit_rf2
importance(fit_rf2)
# Create data frame to store ntree solving results
ntree_df <- data.frame(matrix(0, nrow= 50, ncol = 4))
for (ntree_times in 1:15){
ntree <- 500+ntree_times*10
index_h <- 2 * ntree_times
fit_temp <- randomForest(as.factor(y)~., data= data1, ntree = ntree)
ntree_df[c((index_h-1), index_h),] <- fit_temp[5]
ntree_df[c((index_h-1), index_h), 4] <- ntree
}
head(ntree_df)
fit_temp[5]
dim(fit_temp[5])
dims(fit_temp[5])
fit_temp[5][1]
fit_temp[5][1,1
]
names(fit_temp)
fit_temp$confusion[1]
fit_temp$confusion[2]
fit_temp$confusion[3]
fit_temp$confusion[6]
fit_temp$confusion
ntree_df <- data.frame(matrix(0, nrow= 50, ncol = 2))
names(ntree_df)[1:2] <- ("Accuracy", "N Tree")
names(ntree_df)[1:2] <-c("Accuracy", "N Tree")
ntree_df <- data.frame(matrix(0, nrow= 50, ncol = 2))
names(ntree_df)[1:2] <-c("Accuracy", "N Tree")
for (ntree_times in 1:15){
ntree <- 500+ntree_times*10
index_h <- 2 * ntree_times
fit_temp <- randomForest(as.factor(y)~., data= data1, ntree = ntree)
error_sum <- fit_temp$confusion[5] + fit_temp$confusion[6]
ntree_df[ntree_times, 1] <- 1-error_sum # accuracy
ntree_df[ntree_times,2] <- ntree
}
plot(ntre)
plot(ntree_df)
ntree_df <- data.frame(matrix(0, nrow= 50, ncol = 2))
names(ntree_df)[1:2] <-c("Accuracy", "N Tree")
for (ntree_times in 1:50){
ntree <- 500+ntree_times*10
index_h <- 2 * ntree_times
fit_temp <- randomForest(as.factor(y)~., data= data1, ntree = ntree)
error_sum <- fit_temp$confusion[5] + fit_temp$confusion[6]
ntree_df[ntree_times, 1] <- 1-error_sum # accuracy
ntree_df[ntree_times,2] <- ntree
}
plot(ntree_df)
?rfcv
fit4 < rfcv(data_rf,y )
fit2 <- glm(CRED_APPROVED~. , family = "binomial", data=train)
# Predict ratios on test set
predict_fit2 <- predict(fit2, newdata=test, type = "response")
hist(predict_fit2, breaks= 20)
# Use 50% threshold for predictions ( we can try different thresholds later )
results2  <- ifelse(predict_fit2 > 0.5,'YES','NO')
# Compare with the original results
misClasificError <- mean(results2 != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Confusion matrix
table(test$CRED_APPROVED, predict_fit2>0.5)
fit4 <- rfcv(data_rf,y )
names(fit4)
?rfcv
fit4$error.cv
mean(fit4$error.cv)
ntree_df <- data.frame(matrix(0, nrow= 50, ncol = 2))
names(ntree_df)[1:2] <-c("Accuracy", "N Tree")
for (ntree_times in 1:50){
ntree <- 500+ntree_times*10
index_h <- 2 * ntree_times
fit_temp <- rfcv(data_rf, y , ntree = ntree)
error_sum <- mean(fit_temp$error.cv)
ntree_df[ntree_times, 1] <- 1-error_sum # accuracy
ntree_df[ntree_times,2] <- ntree
}
for (ntree_times in 1:10){
ntree <- 500+ntree_times*10
index_h <- 2 * ntree_times
fit_temp <- rfcv(data_rf, y , ntree = ntree)
error_sum <- mean(fit_temp$error.cv)
ntree_df[ntree_times, 1] <- 1-error_sum # accuracy
ntree_df[ntree_times,2] <- ntree
}
plot(ntree_df)
summary(data$CRED_HERE)
plot(ntree_df[ntree_df!=0])
plot(ntree_df[ntree_df!=0,])
fit7 <- rfcv(CRED_APPROVED ~., data = train)
x <- train[,!CRED_APPROVED]
x <- train[,"CRED_APPROVED"]
x <- train[,!"CRED_APPROVED"]
?subset
x <- subset(train, !names(train) %in% "CRED_APPROVED")
x <- train
x <- model.matrix(train)
x <- model.matrix(CRED_APPROVED~., data= train)
names(x)
View(x)
View(x)
fit7 <- rfcv(y~,x)
fit7 <- rfcv(y,x)
?rfcv
fit7 <- rfcv(x,y)
fit7
names(fit7)
fit7.n.var
fit7$n.var
fit7$error.cv
# Lasso attempt
setwd("C:/Users/board/Desktop/Kaggle/mf850_extra")
data <- read.csv("mf850-loan-data.csv")
set.seed(1)
# Data cleaning - change variables into categorical variables
data$DEPENDENTS <- as.factor(data$DEPENDENTS)
data$CRED_HERE <- as.factor(data$CRED_HERE)
data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
# Scale continous variables
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
# Splitting the data into test/ train sets
# https://ragrawal.wordpress.com/2012/01/14/dividing-data-into-training-and-testing-dataset-in-r/
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
library(glmnet) # glmnet for ridge/ lasso regression
# Split train data into independent and dependent variables
# model matrix splits data into dummy variables
x = model.matrix(CRED_APPROVED~., data = train)
y = ifelse(train$CRED_APPROVED== 'YES',1,0)
# Use cross validation with to determine the lambda parameter for lasso regression
lambdas <- 10^seq(3,-5, length = 100)
cv.lasso = cv.glmnet(x, y, family = "binomial", lambda = lambdas, alpha = 1, standardize = FALSE)
# Plot lambda vs. cross validation error mean
lasso_lambdas <- cv.lasso$lambda
lasso_cv_means <- cv.lasso$cvm
plot(lasso_lambdas, lasso_cv_means, main = "Lasso Lambda vs Cross Validation Error mean")
# should the lambda with the lowest cross validation error rate
(bestlam = cv.lasso$lambda.min)
# look at coefficients which are not 0 of lasso regression
lasso1 <- glmnet(x, y, family = "binomial", alpha = 1, standardize = FALSE, lambda = lasso_lambdas)
lasso.coef <- predict(lasso1, type = "coefficients", s= bestlam)[1:(ncol(x)) ,]
lasso.coef
lasso.coef[lasso.coef!=0]
length(lasso.coef[lasso.coef!=0])
test1 <- model.matrix(CRED_APPROVED~., family = "binomial", data=test)
predict1 <- predict(lasso1, newx= test1, type = "response", s= bestlam)
hist(predict1, breaks= 20)
results2  <- ifelse(predict1 > 0.5,'YES','NO')
# Compare with the original results
misClasificError <- mean(results2 != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Confusion matrix
table(test$CRED_APPROVED, predict1>0.5)
predict_lasso <- predict(lasso1, newx= test1, type = "response", s= bestlam)
hist(predict_lasso, breaks= 20)
predict_lasso  <- ifelse(predict_lasso > 0.5,'YES','NO')
# Compare with the original results
misClasificError <- mean(predict_lasso != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Confusion matrix
table(test$CRED_APPROVED, predict_lasso>0.5)
# Ridge
# Ridge attempt
setwd("C:/Users/board/Desktop/Kaggle/mf850_extra")
data <- read.csv("mf850-loan-data.csv")
set.seed(1)
# Data cleaning - change variables into categorical variables
data$DEPENDENTS <- as.factor(data$DEPENDENTS)
data$CRED_HERE <- as.factor(data$CRED_HERE)
data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
# Scale continous variables
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
# Splitting the data into test/ train sets
# https://ragrawal.wordpress.com/2012/01/14/dividing-data-into-training-and-testing-dataset-in-r/
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
library(glmnet) # glmnet for ridge/ ridge regression
# Split train data into independent and dependent variables
# model matrix splits data into dummy variables
x = model.matrix(CRED_APPROVED~., family = "binomial", data = train)
y = ifelse(train$CRED_APPROVED == 'YES',1,0)
# Use cross validation with to determine the lambda parameter for ridge regression
lambdas <- 10^seq(4,-5, length = 100)
cv.ridge = cv.glmnet(x, y, family = "binomial", lambda = lambdas, alpha = 0, standardize = FALSE)
# Plot lambda vs. cross validation error mean
ridge_lambdas <- cv.ridge$lambda
ridge_cv_means <- cv.ridge$cvm
plot(ridge_lambdas, ridge_cv_means, main = "ridge Lambda vs Cross Validation Error mean")
# should the lambda with the lowest cross validation error rate
bestlam = cv.ridge$lambda.min
# look at coefficients which are not 0 of ridge regression
ridge1 <- glmnet(x, y, family = "binomial", alpha = 0, standardize = FALSE, lambda = lambdas)
ridge.coef <- predict(ridge1, type = "coefficients", s= bestlam)[1:(ncol(x)) ,]
ridge.coef
ridge.coef[abs(ridge.coef) > 0.001]
length(ridge.coef[abs(ridge.coef) > 0.1])
test1 <- model.matrix(CRED_APPROVED~., family = "binomial", data=test)
predict1 <- predict(ridge1, newx= test1, type = "response", s= bestlam)
hist(predict1, breaks= 20)
results2  <- ifelse(predict1 > 0.5,'YES','NO')
# Compare with the original results
misClasificError <- mean(results2 != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Confusion matrix
table(test$CRED_APPROVED, predict1>0.5)
