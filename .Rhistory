data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
head(data)
summary(data)
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
library(randomForest)
set.seed(1)
# Fit random forest on training data
fit_rf1 <- randomForest(CRED_APPROVED~. , data = train)
# Predict outcomes on test data
predict_rf <- predict(fit_rf1, newdata= test)
misClasificError <- mean(predict_rf != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
#Baseline
table(test$CRED_APPROVED)/nrow(test)
x = model.matrix(CRED_APPROVED~., data = train)
fit_temp <- rfcv(x, y , ntree = ntree)
error_sum <- (fit_temp$error.cv)
# Confusion matrix
library(caret)
# http://dni-institute.in/blogs/random-forest-using-r-step-by-step-tutorial/
y <- train$CRED_APPROVED
x <- model.matrix(CRED_APPROVED~., data= train)
fit7 <- rfcv(x,y)
fit7$error.cv
for (ntree_times in 1:6){
ntree <- 500+ntree_times*30
fit_temp <- rfcv(x, y , ntree = ntree)
error_sum <- mean(fit_temp$error.cv)
ntree_df[ntree_times, 1] <- 1-error_sum # accuracy
ntree_df[ntree_times,2] <- ntree
}
plot(ntree_df[ntree_df!=0,])
# See the effect of the number of trees on the feature selection by cross validation
numb_tries <- 6
# Pre allocate space
ntree_df <- data.frame(matrix(0, nrow= numb_tries, ncol = 7))
names(ntree_df)[1:2] <-c("N Tree", "Accuracy")
#
for (ntree_times in 1:numb_tries){
ntree <- 500+ntree_times*30
fit_temp <- rfcv(x, y , ntree = ntree)
error_sum <- fit_temp$error.cv
ntree_df[ntree_times, 2:6] <- 1-error_sum # accuracy
ntree_df[ntree_times,1] <- ntree
}
min(ntree_df)
for (ntree_times in 1:numb_tries){
ntree <- 500+ntree_times*30
fit_temp <- rfcv(x, y , ntree = ntree)
error_sum <- fit_temp$error.cv
ntree_df[ntree_times, 2:7] <- 1-error_sum # accuracy
ntree_df[ntree_times,1] <- ntree
}
min(ntree_df)
max(ntree_df[,2:6])
ntree_df
?which
misClasificError <- mean(results2 == test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Ridge
# Ridge attempt
setwd("C:/Users/board/Desktop/Kaggle/mf850_extra")
data <- read.csv("mf850-loan-data.csv")
set.seed(1)
# Data cleaning - change variables into categorical variables
data$DEPENDENTS <- as.factor(data$DEPENDENTS)
data$CRED_HERE <- as.factor(data$CRED_HERE)
data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
# Scale continous variables
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
# Splitting the data into test/ train sets
# https://ragrawal.wordpress.com/2012/01/14/dividing-data-into-training-and-testing-dataset-in-r/
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
library(glmnet) # glmnet for ridge/ ridge regression
# Split train data into independent and dependent variables
# model matrix splits data into dummy variables
x = model.matrix(CRED_APPROVED~., family = "binomial", data = train)
y = ifelse(train$CRED_APPROVED == 'YES',1,0)
# Use cross validation with to determine the lambda parameter for ridge regression
lambdas <- 10^seq(4,-5, length = 100)
cv.ridge = cv.glmnet(x, y, family = "binomial", lambda = lambdas, alpha = 0, standardize = FALSE)
# Plot lambda vs. cross validation error mean
ridge_lambdas <- cv.ridge$lambda
ridge_cv_means <- cv.ridge$cvm
plot(ridge_lambdas, ridge_cv_means, main = "ridge Lambda vs Cross Validation Error mean")
# should the lambda with the lowest cross validation error rate
bestlam = cv.ridge$lambda.min
# look at coefficients which are not 0 of ridge regression
ridge1 <- glmnet(x, y, family = "binomial", alpha = 0, standardize = FALSE, lambda = lambdas)
ridge.coef <- predict(ridge1, type = "coefficients", s= bestlam)[1:(ncol(x)) ,]
ridge.coef
ridge.coef[abs(ridge.coef) > 0.001]
length(ridge.coef[abs(ridge.coef) > 0.1])
# Test on test set
# Prepare the test set data
test1 <- model.matrix(CRED_APPROVED~., family = "binomial", data=test)
# Make predictions using ridge model on test data
predict_ridge <- predict(ridge1, newx= test1, type = "response", s= bestlam)
# Look at predictions
hist(predict_ridge, breaks= 20)
predict_ridge  <- ifelse(predict_ridge > 0.5,'YES','NO')
# Compare with the original results
misClasificError <- mean(predict_ridge != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Compare with Baseline
table(test$CRED_APPROVED)/nrow(test)
dim(predict_ridge)
predict_ridge_correct <- predict_ridge[predict_ridge == test$CRED_APPROVED]
hist(predict_ridge_correct)
# Prepare the test set data
test1 <- model.matrix(CRED_APPROVED~., family = "binomial", data=test)
# Make predictions using ridge model on test data
predict_ridge <- predict(ridge1, newx= test1, type = "response", s= bestlam)
# Look at predictions
hist(predict_ridge, breaks= 20)
# Re format results so they will be comparable
predict_ridge_yn <- ifelse(predict_ridge > 0.5,'YES','NO')
# Compare with the original results
misClasificError <- mean(predict_ridge_yn != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Compare with Baseline
table(test$CRED_APPROVED)/nrow(test)
predict_ridge_correct <- predict_ridge[predict_ridge_yn == test$CRED_APPROVED]
hist(predict_ridge_correct)
predict_ridge_incorrect <- predict_ridge[predict_ridge_yn != test$CRED_APPROVED]
hist(predict_ridge_incorrect)
# Stepwise Variable selection
setwd("C:/Users/board/Desktop/Kaggle/mf850_extra")
data <- read.csv("mf850-loan-data.csv")
set.seed(1)
# Data cleaning - change variables into categorical variables
data$DEPENDENTS <- as.factor(data$DEPENDENTS)
data$CRED_HERE <- as.factor(data$CRED_HERE)
data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
# Scale continous variables
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
# Splitting the data into test/ train sets
# https://ragrawal.wordpress.com/2012/01/14/dividing-data-into-training-and-testing-dataset-in-r/
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
# Try backwards stepwise regression with logistic regression
# Inspiration
# http://www.utstat.toronto.edu/~brunner/oldclass/appliedf11/handouts/2101f11StepwiseLogisticR.pdf
# Logistic regression with all variables
fullmod <- glm(CRED_APPROVED~. , family = "binomial", data =train)
summary(fullmod)
# Logisitic regression with no variables
nothing <- glm(CRED_APPROVED~1, family = "binomial", data = train)
summary(nothing) # Essentially intercept says yes (it is greater than 0.5)
# Backward stepwise regression
backwards <- step(fullmod, direction = "backward", trace = 0)
summary(backwards)
length(backwards$coefficients)
# Great only 35 variables compared to 56
# Let's see how it does on the prediction set
predict_fit2 <- predict(backwards, newdata=test, type = "response")
hist(predict_fit2, breaks = 20)
# Use 50% threshold for predictions ( we can try different thresholds later )
results2  <- ifelse(predict_fit2 > 0.5,'YES','NO')
# Compare with the original results
misClasificError <- mean(results2 != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Compare with Baseline
table(test$CRED_APPROVED)/nrow(test)
# Ridge
# Ridge attempt
setwd("C:/Users/board/Desktop/Kaggle/mf850_extra")
data <- read.csv("mf850-loan-data.csv")
set.seed(1)
# Data cleaning - change variables into categorical variables
data$DEPENDENTS <- as.factor(data$DEPENDENTS)
data$CRED_HERE <- as.factor(data$CRED_HERE)
data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
# Scale continous variables
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
# Splitting the data into test/ train sets
# https://ragrawal.wordpress.com/2012/01/14/dividing-data-into-training-and-testing-dataset-in-r/
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
library(glmnet) # glmnet for ridge/ ridge regression
# Split train data into independent and dependent variables
# model matrix splits data into dummy variables
x = model.matrix(CRED_APPROVED~., family = "binomial", data = train)
y = ifelse(train$CRED_APPROVED == 'YES',1,0)
# Use cross validation with to determine the lambda parameter for ridge regression
lambdas <- 10^seq(4,-5, length = 100)
cv.ridge = cv.glmnet(x, y, family = "binomial", lambda = lambdas, alpha = 0, standardize = FALSE)
# Plot lambda vs. cross validation error mean
ridge_lambdas <- cv.ridge$lambda
ridge_cv_means <- cv.ridge$cvm
plot(ridge_lambdas, ridge_cv_means, main = "ridge Lambda vs Cross Validation Error mean")
# should the lambda with the lowest cross validation error rate
bestlam = cv.ridge$lambda.min
# look at coefficients which are not very small for the ridge regression
ridge1 <- glmnet(x, y, family = "binomial", alpha = 0, standardize = FALSE, lambda = lambdas)
ridge.coef <- predict(ridge1, type = "coefficients", s= bestlam)[1:(ncol(x)) ,]
ridge.coef
ridge.coef[abs(ridge.coef) > 0.001]
length(ridge.coef[abs(ridge.coef) > 0.1])
# Test on test set
# Prepare the test set data
test1 <- model.matrix(CRED_APPROVED~., family = "binomial", data=test)
# Make predictions using ridge model on test data
predict_ridge <- predict(ridge1, newx= test1, type = "response", s= bestlam)
# Look at predictions
hist(predict_ridge, breaks= 20)
# Re format results so they will be comparable
predict_ridge_yn <- ifelse(predict_ridge > 0.5,'YES','NO')
# Compare with the original results
misClasificError <- mean(predict_ridge_yn != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Compare with Baseline
table(test$CRED_APPROVED)/nrow(test)
# Quick error analysis
# Probabilities where the model predicted correctly
predict_ridge_correct <- predict_ridge[predict_ridge_yn == test$CRED_APPROVED]
hist(predict_ridge_correct)
# Probabilities where the model predicted incorrectly
predict_ridge_incorrect <- predict_ridge[predict_ridge_yn != test$CRED_APPROVED]
hist(predict_ridge_incorrect)
# Lasso attempt
setwd("C:/Users/board/Desktop/Kaggle/mf850_extra")
data <- read.csv("mf850-loan-data.csv")
set.seed(1)
# Data cleaning - change variables into categorical variables
data$DEPENDENTS <- as.factor(data$DEPENDENTS)
data$CRED_HERE <- as.factor(data$CRED_HERE)
data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
# Scale continous variables
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
# Splitting the data into test/ train sets
# https://ragrawal.wordpress.com/2012/01/14/dividing-data-into-training-and-testing-dataset-in-r/
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
library(glmnet) # glmnet for ridge/ lasso regression
# Split train data into independent and dependent variables
# model matrix splits data into dummy variables
x = model.matrix(CRED_APPROVED~., data = train)
y = ifelse(train$CRED_APPROVED== 'YES',1,0)
# Use cross validation with to determine the lambda parameter for lasso regression
lambdas <- 10^seq(3,-5, length = 100)
cv.lasso = cv.glmnet(x, y, family = "binomial", lambda = lambdas, alpha = 1, standardize = FALSE)
# Plot lambda vs. cross validation error mean
lasso_lambdas <- cv.lasso$lambda
lasso_cv_means <- cv.lasso$cvm
plot(lasso_lambdas, lasso_cv_means, main = "Lasso Lambda vs Cross Validation Error mean")
# should the lambda with the lowest cross validation error rate
(bestlam = cv.lasso$lambda.min)
# look at coefficients of the lasso regression
lasso1 <- glmnet(x, y, family = "binomial", alpha = 1, standardize = FALSE, lambda = lasso_lambdas)
lasso.coef <- predict(lasso1, type = "coefficients", s= bestlam)[1:(ncol(x)) ,]
lasso.coef
# Look at the coefficients which are not 0 and count the number of them
lasso.coef[lasso.coef!=0]
length(lasso.coef[lasso.coef!=0])
# Test model on new data
# Make predictions on new data
predict_lasso <- predict(lasso1, newx= test1, type = "response", s= bestlam)
# Histogram of probabilities
hist(predict_lasso, breaks= 20)
# Change predictions so they are comparable to test file
predict_lasso  <- ifelse(predict_lasso > 0.5,'YES','NO')
# Find the accuracy by comparing predictions with test results
misClasificError <- mean(predict_lasso != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Compare with Baseline
table(test$CRED_APPROVED)/nrow(test)
# Test model on new data
# Prepare the test set data
test1 <- model.matrix(CRED_APPROVED~., family = "binomial", data=test)
# Make predictions on new data
predict_lasso <- predict(lasso1, newx= test1, type = "response", s= bestlam)
# Histogram of probabilities
hist(predict_lasso, breaks= 20)
# Change predictions so they are comparable to test file
predict_lasso  <- ifelse(predict_lasso > 0.5,'YES','NO')
# Find the accuracy by comparing predictions with test results
misClasificError <- mean(predict_lasso != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Compare with Baseline
table(test$CRED_APPROVED)/nrow(test)
# Remember to change the working directory to the appropriate place on your computer- where ever the data is located
# on the top line (where file, edit ...) - go to session -> set working directory -> choose directory
# Then change this line with the appropriate one
setwd("C:/Users/board/Desktop/Kaggle/mf850_extra")
data <- read.csv("mf850-loan-data.csv")
# Data cleaning - change variables into categorical variables
data$DEPENDENTS <- as.factor(data$DEPENDENTS)
data$CRED_HERE <- as.factor(data$CRED_HERE)
data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
# Scale continous variables
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
# Splitting the data into test/ train sets
# https://ragrawal.wordpress.com/2012/01/14/dividing-data-into-training-and-testing-dataset-in-r/
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
# Baseline of only guessing yes
(sum(data$CRED_APPROVED == 'YES')/ length(data$CRED_APPROVED))
table(data$CRED_APPROVED)/nrow(data)
# Fit logisitic regression on test set
fit2 <- glm(CRED_APPROVED~. , family = "binomial", data=train)
# Predict ratios on test set
predict_fit2 <- predict(fit2, newdata=test, type = "response")
hist(predict_fit2, breaks= 20)
# Use 50% threshold for predictions ( we can try different thresholds later )
results2  <- ifelse(predict_fit2 > 0.5,'YES','NO')
# Compare with the original results
misClasificError <- mean(results2 != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
# Confusion matrix
table(test$CRED_APPROVED, predict_fit2>0.5)
#Baseline
table(test$CRED_APPROVED)/nrow(test)
# Remember to change the working directory to the appropriate place on your computer- where ever the data is located
# on the top line (where file, edit ...) - go to session -> set working directory -> choose directory
# Then change this line with the appropriate one
setwd("C:/Users/board/Desktop/Kaggle/mf850_extra")
data <- read.csv("mf850-loan-data.csv")
# Data cleaning - change variables into categorical variables
data$DEPENDENTS <- as.factor(data$DEPENDENTS)
data$CRED_HERE <- as.factor(data$CRED_HERE)
data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
# Scale continous variables
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
# Splitting the data into test/ train sets
# https://ragrawal.wordpress.com/2012/01/14/dividing-data-into-training-and-testing-dataset-in-r/
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
# Baseline of only guessing yes
(sum(data$CRED_APPROVED == 'YES')/ length(data$CRED_APPROVED))
table(data$CRED_APPROVED)/nrow(data)
# Fit logisitic regression on test set
fit2 <- glm(CRED_APPROVED~. , family = "binomial", data=train)
# Predict ratios on test set
predict_fit2 <- predict(fit2, newdata=test, type = "response")
hist(predict_fit2, breaks= 20)
# Use 50% threshold for predictions ( we can try different thresholds later )
results2  <- ifelse(predict_fit2 > 0.5,'YES','NO')
# Accuracy on test data
misClasificError <- mean(results2 != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
#Baseline
table(test$CRED_APPROVED)/nrow(test)
setwd("C:/Users/board/Desktop/Kaggle/mf850_extra")
data <- read.csv("mf850-loan-data.csv")
# Some data cleaning
data$DEPENDENTS <- as.factor(data$DEPENDENTS)
data$CRED_HERE <- as.factor(data$CRED_HERE)
data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
head(data)
summary(data)
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
library(randomForest)
set.seed(1)
# Fit random forest on training data
fit_rf1 <- randomForest(CRED_APPROVED~. , data = train)
# Predict outcomes on test data
predict_rf <- predict(fit_rf1, newdata= test)
misClasificError <- mean(predict_rf != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
#Baseline
table(test$CRED_APPROVED)/nrow(test)
fit_temp
# Prepare data
y <- train$CRED_APPROVED
x <- model.matrix(CRED_APPROVED~., data= train)
# Fit the model
fit_temp <- rfcv(x, y , ntree = ntree)
# Cross validation error rate by number of features included in random forest
error <- (fit_temp$error.cv)
# Potential RF guide
# http://dni-institute.in/blogs/random-forest-using-r-step-by-step-tutorial/
# See the effect of the number of trees on the feature selection by cross validation
numb_tries <- 6
# Pre allocate space
ntree_df <- data.frame(matrix(0, nrow= numb_tries, ncol = 7))
names(ntree_df)[1:2] <-c("N Tree", "Accuracy")
# Check the effect of the number of trees on the feature selection technique
for (ntree_times in 1:numb_tries){
ntree <- 500+ntree_times*30
fit_temp <- rfcv(x, y , ntree = ntree)
error_sum <- fit_temp$error.cv
ntree_df[ntree_times, 2:7] <- 1-error_sum # accuracy
ntree_df[ntree_times,1] <- ntree
}
max(ntree_df[,2:6])
?which
which(ntree_df == max(ntree_df[,2:6]))
ntree_df[which(ntree_df == max(ntree_df[,2:6]))]
ntree_df[which(ntree_df[,2:6] == max(ntree_df[,2:6]))]
ntrre_df
ntree_df
max(ntree_df[,2:6])
setwd("C:/Users/board/Desktop/Kaggle/mf850_extra")
data <- read.csv("mf850-loan-data.csv")
# Some data cleaning
data$DEPENDENTS <- as.factor(data$DEPENDENTS)
data$CRED_HERE <- as.factor(data$CRED_HERE)
data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
head(data)
summary(data)
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
library(randomForest)
set.seed(1)
# Fit random forest on training data
fit_rf1 <- randomForest(CRED_APPROVED~. , data = train)
# Predict outcomes on test data
predict_rf <- predict(fit_rf1, newdata= test)
misClasificError <- mean(predict_rf != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
#Baseline
table(test$CRED_APPROVED)/nrow(test)
?randomForest
fit_temp <- randomForest(x,y,ntree= 600)
fit_temp[5]
max(ntree_df[,2:6])
?rfcv
max(ntree_df[,2:6])
ntree(12)
ntree_df[12]
setwd("C:/Users/board/Desktop/Kaggle/mf850_extra")
data <- read.csv("mf850-loan-data.csv")
# Some data cleaning
data$DEPENDENTS <- as.factor(data$DEPENDENTS)
data$CRED_HERE <- as.factor(data$CRED_HERE)
data$INSTALLMENTRATE <- as.factor(data$INSTALLMENTRATE)
data$ATADDRESSSINCE <- as.factor(data$ATADDRESSSINCE)
data$AGE <- scale(data$AGE)
data$DURATION <- scale(data$DURATION)
head(data)
summary(data)
# Sample split into test train set
indexes <- sample(1:nrow(data), size = 0.2*nrow(data))
test <- data[indexes, ]
train <- data[-indexes, ]
library(randomForest)
set.seed(1)
# Fit random forest on training data
fit_rf1 <- randomForest(CRED_APPROVED~. , data = train)
# Predict outcomes on test data
predict_rf <- predict(fit_rf1, newdata= test)
misClasificError <- mean(predict_rf != test$CRED_APPROVED)
print(paste('Accuracy',1-misClasificError))
#Baseline
table(test$CRED_APPROVED)/nrow(test)
# Try Random forest feature selection by cross validation
# Prepare data
y <- train$CRED_APPROVED
x <- model.matrix(CRED_APPROVED~., data= train)
# Fit the model
fit_temp <- rfcv(x, y , ntree = ntree)
# Cross validation error rate by number of features included in random forest
error <- (fit_temp$error.cv)
# Potential RF guide
# http://dni-institute.in/blogs/random-forest-using-r-step-by-step-tutorial/
# See the effect of the number of trees on the feature selection by cross validation
numb_tries <- 6
# Pre allocate space
ntree_df <- data.frame(matrix(0, nrow= numb_tries, ncol = 7))
names(ntree_df)[1:2] <-c("N Tree", "Accuracy")
# Check the effect of the number of trees on the feature selection technique
for (ntree_times in 1:numb_tries){
ntree <- 500+ntree_times*30
fit_temp <- rfcv(x, y , ntree = ntree)
error_sum <- fit_temp$error.cv
ntree_df[ntree_times, 2:7] <- 1-error_sum # accuracy
ntree_df[ntree_times,1] <- ntree
}
max(ntree_df[,2:6])
